**This repository is now obsolete. For a more up to date demo on SCDF please check https://github.com/jaimegag/scdf-zone**

= PCF-scdf-demo

Based on the original repo created by Adam Zwickey https://github.com/azwickey-pivotal/scdf-raven

== Prepare Env

. Download PCFDev, a full, local install of Cloudfoundry here: https://network.pivotal.io/products/pcfdev

. Install PCFDev following the documentation instructions here: http://docs.pivotal.io/pcf-dev/index.html

. Log into PCFDev using CF CLI
+
[source,bash]
---------------------------------------------------------------------
$ cf login -a api.local.pcfdev.io --skip-ssl-validation -u admin -p admin
---------------------------------------------------------------------

. Create the required RabbitMQ, MySQL and Redis service instances
+
[source,bash]
---------------------------------------------------------------------
$ cf create-service p-rabbitmq standard rabbit
$ cf create-service p-mysql 512mb mysql
$ cf create-service p-redis shared-vm redis
---------------------------------------------------------------------

== Deploy Spring Cloud Dataflow Server

. Change directories to the _scdf-server_ directory

. Deploy the Spring Cloud Dataflow server to Cloudfoundry.
(Note that there is no need to build the scdf-server as the manifest is using a pre-built jar that ihas been conveniently place in the root folder of the scdf-server)
+
[source,bash]
---------------------------------------------------------------------
$ cf push
---------------------------------------------------------------------

. After the application starts up you will have dataflow running.  If you'd like, you cann access the Dataflow UI here: http://scdf-server.local.pcfdev.io/dashboard

. By Default you will not have any Stream applications available.  We'll use the Spring Cloud Dataflow shell to load them.

. Change directories to the _scdf-shell_ directory and run the SCDF shell:
+
[source,bash]
---------------------------------------------------------------------
$ mvn clean spring-boot:run
or
$ java -jar spring-cloud-dataflow-shell-1.0.0.RELEASE.jar
---------------------------------------------------------------------

. After the Shell loads we need to target our deployed dataflow server:
+
[source,bash]
---------------------------------------------------------------------
  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/

1.0.0.M3

Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".


server-unknown:> dataflow config server http://scdf-server.local.pcfdev.io
Successfully targeted http://scdf-server.local.pcfdev.io

---------------------------------------------------------------------

.  Your shell should now be targeted correctly.  Test this by listing the current streams (which should be empty right now):
+
[source,bash]
---------------------------------------------------------------------
dataflow:>stream list
╔═══════════╤═════════════════╤══════╗
║Stream Name│Stream Definition│Status║
╚═══════════╧═════════════════╧══════╝

---------------------------------------------------------------------

. Load the out-of-the-box Stream applications by executing this command.
+
[source,bash]
---------------------------------------------------------------------
dataflow:>app import --uri http://bit.ly/stream-applications-rabbit-maven

Successfully registered applications: [source.tcp, sink.jdbc, source.http, sink.rabbit, source.rabbit, source.ftp, sink.gpfdist, processor.transform, source.sftp, processor.filter, source.file, sink.cassandra, processor.groovy-filter, sink.router, source.trigger, sink.hdfs-dataset, processor.splitter, source.load-generator, processor.tcp-client, sink.file, source.time, source.gemfire, source.twitterstream, sink.tcp, source.jdbc, sink.field-value-counter, sink.redis-pubsub, sink.hdfs, processor.bridge, processor.pmml, processor.httpclient, sink.ftp, source.s3, sink.log, sink.gemfire, sink.aggregate-counter, sink.throughput, source.triggertask, sink.s3, source.gemfire-cq, source.jms, source.tcp-client, processor.scriptable-transform, sink.counter, sink.websocket, source.mongodb, source.mail, processor.groovy-transform, source.syslog]

---------------------------------------------------------------------

. List the loaded applications using the shell:
+
[source,bash]
---------------------------------------------------------------------
dataflow:>app list
╔══════════════╤════════════════════╤═══════════════════╤═════════╗
║    source    │     processor      │       sink        │  task   ║
╠══════════════╪════════════════════╪═══════════════════╪═════════╣
║file          │bridge              │aggregate-counter  │timestamp║
║ftp           │filter              │cassandra          │         ║
║http          │groovy-filter       │counter            │         ║
║jdbc          │groovy-transform    │field-value-counter│         ║
║jms           │httpclient          │file               │         ║
║load-generator│pmml                │ftp                │         ║
║rabbit        │scriptable-transform│gemfire            │         ║
║sftp          │splitter            │gpfdist            │         ║
║tcp           │transform           │hdfs               │         ║
║time          │                    │jdbc               │         ║
║trigger       │                    │log                │         ║
║twitterstream │                    │rabbit             │         ║
║              │                    │redis              │         ║
║              │                    │router             │         ║
║              │                    │tcp                │         ║
║              │                    │throughput         │         ║
║              │                    │websocket          │         ║
╚══════════════╧════════════════════╧═══════════════════╧═════════╝

---------------------------------------------------------------------

== Load Custom Applications and Create a Stream using the Spring Cloud Dataflow Shell

. First we'll load our custom apps that are found in the _/scdf-source_, _/scdf-sink_ and  _/scdf-mysqlsink_ directories.  The code is already compiled and Spring Cloud Dataflow will download these from GIT over HTTP.  Execute the command in the Spring Cloud Dataflow shell:
+
[source,bash]
---------------------------------------------------------------------
dataflow:>app import --uri file://$LOCAL_FILESYSTEM_PATH/scdf-shell/app_custom.properties --local TRUE
Successfully registered applications: [sink.sink-sample, source.source-sample, sink.sink-mysqlsample]
---------------------------------------------------------------------

. Create your first stream using two of these apps.  Execute the following command in the Spring Cloud Dataflwo shell:
+
[source,bash]
---------------------------------------------------------------------
dataflow:>stream create --name test --definition "source-sample | sink-sample" --deploy
Created and deployed new stream 'test'

---------------------------------------------------------------------

. In a separate terminal window check the status of the stream deployment using the CF CLI.  The app names will be prefixed with _dataflow-test_, which is the name of your stream:
+
[source,bash]
---------------------------------------------------------------------
$ cf apps
Getting apps in org pcfdev-org / space pcfdev-space as admin...
OK

name                             requested state   instances   memory   disk   urls
scdf-server                      started           1/1         1G       512M   scdf-server.local.pcfdev.io
dataflow-test-sink-sample        started           1/1         1G       1G     dataflow-test-sink-sample.local.pcfdev.io
dataflow-test-source-sample      started           1/1         1G       1G     dataflow-test-source-sample.local.pcfdev.io

---------------------------------------------------------------------
. You can also access the CF DataFlow Server Dashboard to create, deploy and manage all streams and apps from the browser:
+
http://scdf-server.local.pcfdev.io/dashboard

. Tail the logs of the dataflow-test-sink-sample CF application:
+
[source,bash]
---------------------------------------------------------------------
$ $ cf logs dataflow-test-sink-sample
  Connected, tailing logs for app dataflow-test-sink-sample in org pcfdev-org / space pcfdev-space as admin...

---------------------------------------------------------------------

. The _dataflow-test-source-sample_ application is listening at an /event endpoint.  Hit this endpoint using curl:
+
[source,bash]
---------------------------------------------------------------------
$ curl -k https://dataflow-test-source-sample.local.pcfdev.io/event\?msg\=I%20just%20created%20a%20data%20stream
event[I just created a data stream] placed on streaming bus%
---------------------------------------------------------------------

. Check the logs of the dataflow-test-sink-sample application (they should already be tailing in one of your windows).  You'll see the message you just posted:
+
[source,bash]
---------------------------------------------------------------------
$ cf logs dataflow-test-sink-sample
Connected, tailing logs for app dataflow-test-sink-sample in org pcfdev-org / space pcfdev-space as admin...

2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : I just created a data stream
2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : 	amqp_receivedRoutingKey=test.source-sample
2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : 	amqp_receivedExchange=test.source-sample
2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : 	Header1=Sent from data microservice
2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : 	amqp_deliveryTag=4
2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : 	amqp_consumerQueue=test.source-sample.test
2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : 	amqp_redelivered=false
2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : 	id=ef984117-a9e9-bdcf-5810-5be8afc0bb7d
2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : 	amqp_consumerTag=amq.ctag-_RxrssJUrMq6LsDvGAztnQ
2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : 	contentType=text/plain
2016-06-30T09:28:07.94-0400 [APP/0]      OUT 2016-06-30 13:28:07.942  INFO 14 --- [e-sample.test-1] c.p.SCDfMysqlSinkSampleApplication       : 	timestamp=1467293287941

---------------------------------------------------------------------


== Create the new Stream that stores the messages in a MySQL database

. Undeploy the firt stream. This is an optional step, but if you are running this demo in PCFDev, you won't have enough resources to have both streams up and running. Execute the following command in the Spring Cloud Dataflwo shell:
+
[source,bash]
---------------------------------------------------------------------
dataflow:>stream undeploy --name test
Un-deployed stream 'test'

---------------------------------------------------------------------

. We already loaded the all our custom applicatins earlier in this demo. Now we are going to use the  _/scdf-source_ and _/scdf-mysqlsink_ directories.

. Create the second stream executing the following command in the Spring Cloud Dataflwo shell:
+
[source,bash]
---------------------------------------------------------------------
dataflow:>stream create --name testmysql --definition "source-sample | sink-mysqlsample" --deploy
Created and deployed new stream 'testmysql'

---------------------------------------------------------------------

. In a separate terminal window check the status of the stream deployment using the CF CLI.  This time the app names will be prefixed with _dataflow-testmysql_, which is the name of your stream:
+
[source,bash]
---------------------------------------------------------------------
$ cf apps
Getting apps in org pcfdev-org / space pcfdev-space as admin...
OK

name                                 requested state   instances   memory   disk   urls
scdf-server                          started           1/1         1G       512M   scdf-server.local.pcfdev.io
dataflow-testmysql-sink-mysqlsample  started           1/1         1G       1G     dataflow-testmysql-sink-mysqlsample.local.pcfdev.io
dataflow-testmysql-source-sample     started           1/1         1G       1G     dataflow-testmysql-source-sample.local.pcfdev.io

---------------------------------------------------------------------

. Confirm that the _dataflow-testmysql-sink-mysqlsample_ application is bound to the _mysql_ service. This binding happens automatically thanks to the environment variables that we initially setup for the _scdf_server_. Run this command:
+
[source,bash]
---------------------------------------------------------------------
$ cf services
Getting services in org pcfdev-org / space pcfdev-space as admin...
OK

name     service      plan        bound apps                                                                                       last operation
rabbit   p-rabbitmq   standard    scdf-server, dataflow-testmysql-sink-mysqlsample, dataflow-testmysql-source-sample               create succeeded
redis    p-redis      shared-vm   scdf-server, dataflow-testmysql-sink-mysqlsample, dataflow-testmysql-source-sample               create succeeded
mysql    p-mysql      512mb       scdf-server, dataflow-testmysql-sink-mysqlsample, dataflow-testmysql-source-sample               create succeeded

---------------------------------------------------------------------

. Tail the logs of the dataflow-testmysql-sink-mysqlsample CF application:
+
[source,bash]
---------------------------------------------------------------------
$ $ cf logs dataflow-testmysql-sink-mysqlsample
  Connected, tailing logs for app dataflow-testmysql-sink-mysqlsample in org pcfdev-org / space pcfdev-space as admin...

---------------------------------------------------------------------

. As with the previous stream, we will create a message hitting the endpoint of the Source application:
+
[source,bash]
---------------------------------------------------------------------
$ curl -k https://dataflow-testmysql-source-sample.local.pcfdev.io/event\?msg\=I%20just%20created%20a%20data%20stream%20to%20write%20in%20a%20database
event[I just created a data stream to write in a database] placed on streaming bus
---------------------------------------------------------------------

. Now we can check the logs of the dataflow-testmysql-sink-sample application (they should already be tailing in one of your windows).  You'll see a similar message to the one we saw for the first stream.

. Finally we can confirm that the messages were written in the MySQL dataase. To do this you can access it with the _mysql_ command. (Notice the $MYSQL_USER and $MYSQL_DATABASE entries and replace them with the credentials that you can get from the mysql serivce in Apps Manager):
+
[source,bash]
---------------------------------------------------------------------

mysql -h mysql.local.pcfdev.io -u $MYSQL_USER -p
Enter password:
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 288
Server version: 5.5.5-10.0.23-MariaDB-wsrep Source distribution, wsrep_25.11.r21a2415

Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> use $MYSQL_DATABASE
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> select * from message;
+----+-----------------------------------------------------+
| id | message                                             |
+----+-----------------------------------------------------+
|  1 | I just created a data stream to write in a database |
+----+-----------------------------------------------------+
1 rows in set (0.00 sec)

mysql>
---------------------------------------------------------------------
